{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Flatten, \\\n",
    "    Dense, \\\n",
    "    Embedding, \\\n",
    "    BatchNormalization, \\\n",
    "    Dropout, \\\n",
    "    Input, \\\n",
    "    Convolution1D, \\\n",
    "    MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "from utils import load_array\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "idx_word = {idx:word for word, idx in word_index.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\n"
     ]
    }
   ],
   "source": [
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample of first 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx_word[i] for i in x_train[0][0:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data munging for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "n_factors = 50\n",
    "review_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def massage_data(review_list):\n",
    "    return [np.minimum(np.array(review), vocab_size - 1) for review in review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn = massage_data(x_train)\n",
    "val = massage_data(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn = pad_sequences(trn, maxlen=review_length)\n",
    "val = pad_sequences(val, maxlen=review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(input_dim=vocab_size, output_dim=n_factors, input_length=review_length),\n",
    "                    Dense(32, activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dropout(0.7),\n",
    "                    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_10 (Embedding)         (None, 500, 50)       250000      embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 500, 32)       1632        embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 16000)         0           dense_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 16000)         0           flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 1)             16001       dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 267,633\n",
      "Trainable params: 267,633\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 5s - loss: 0.5441 - acc: 0.6892 - val_loss: 0.3170 - val_acc: 0.8661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4c2880fd0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, batch_size=64, nb_epoch=1, validation_data=(val, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 5s - loss: 0.2808 - acc: 0.8839 - val_loss: 0.2748 - val_acc: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4c2880d90>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.1\n",
    "model.fit(trn, labels_train, batch_size=64, nb_epoch=1, validation_data=(val, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Conv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(input_dim=vocab_size, output_dim=n_factors, input_length=review_length),\n",
    "                    Convolution1D(nb_filter=32, filter_length=3),\n",
    "                    Dropout(0.2),\n",
    "                    Dense(100, activation='relu'),\n",
    "                    BatchNormalization(),\n",
    "                    Flatten(),\n",
    "                    Dropout(0.7),\n",
    "                    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 10s - loss: 0.4754 - acc: 0.7427 - val_loss: 0.6557 - val_acc: 0.5087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f544b2a8d90>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, batch_size=128, nb_epoch=1, validation_data=(val, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 9s - loss: 0.1942 - acc: 0.9243 - val_loss: 0.4133 - val_acc: 0.8311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f544ac6b550>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.1\n",
    "model.fit(trn, labels_train, batch_size=128, nb_epoch=1, validation_data=(val, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    }
   ],
   "source": [
    "glove_vecs, glove_words, glove_word_idx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fac = vecs.shape[1]\n",
    "    emb = np.empty((vocab_size, n_fac))\n",
    "    \n",
    "    for idx in range(1, vocab_size):\n",
    "        word = idx_word[idx]\n",
    "        if word and re.search('^[a-zA-Z0-9\\-]*$', word):\n",
    "            emb[idx] = glove_vecs[glove_word_idx[word]]\n",
    "        else:\n",
    "            emb[idx] = np.random.normal(scale=0.6, size=(n_fac,))\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fac,))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(input_dim=vocab_size, output_dim=vecs.shape[1], input_length=review_length, \n",
    "                              weights=[emb], dropout=0.2, trainable=False),\n",
    "                    Convolution1D(nb_filter=64, filter_length=5, activation='relu'),\n",
    "                    MaxPooling1D(),\n",
    "                    Flatten(),\n",
    "                    Dense(100, activation='relu'),\n",
    "                    Dropout(0.7),\n",
    "                    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(vocab_size, vecs.shape[1], input_length=review_length, dropout=0.2, \n",
    "                              weights=[emb], trainable=False),\n",
    "                    Dropout(0.25),\n",
    "                    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "                    Dropout(0.25),\n",
    "                    MaxPooling1D(),\n",
    "                    Flatten(),\n",
    "                    Dense(100, activation='relu'),\n",
    "                    Dropout(0.7),\n",
    "                    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_20 (Embedding)         (None, 500, 50)       250000      embedding_input_20[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 500, 50)       0           embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_17 (Convolution1D) (None, 500, 64)       16064       dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 500, 64)       0           convolution1d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_6 (MaxPooling1D)    (None, 250, 64)       0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)             (None, 16000)         0           maxpooling1d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_39 (Dense)                 (None, 100)           1600100     flatten_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 100)           0           dense_39[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_40 (Dense)                 (None, 1)             101         dropout_7[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,866,265\n",
      "Trainable params: 1,616,265\n",
      "Non-trainable params: 250,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 11s - loss: 0.1989 - acc: 0.6841 - val_loss: 0.1641 - val_acc: 0.7630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4c206ba50>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, nb_epoch=1, validation_data=(val, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.1476 - acc: 0.7866 - val_loss: 0.1288 - val_acc: 0.8214\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.1467 - acc: 0.7895 - val_loss: 0.1282 - val_acc: 0.8264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4c1bc3fd0>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.1\n",
    "model.fit(trn, labels_train, nb_epoch=2, validation_data=(val, labels_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
